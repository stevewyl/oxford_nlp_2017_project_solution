{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 打开文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lxml.etree\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('ted_en-20160408.zip', 'r') as z:\n",
    "    doc = lxml.etree.parse(z.open('ted_en-20160408.xml', 'r'))\n",
    "input_text = doc.xpath('//content/text()')\n",
    "labels = doc.xpath('//keywords/text()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2085"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标签处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据作业要求，我们只抽取包含Technology，Entertainment和Design的文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 关键字抽取模块\n",
    "from flashtext import KeywordProcessor\n",
    "keyword_processor = KeywordProcessor()\n",
    "valid_list = ['technology', 'entertainment', 'design']\n",
    "for word in valid_list:\n",
    "    keyword_processor.add_keyword(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['talks, business, creativity, curiosity, goal-setting, innovation, motivation, potential, success, work',\n",
       " 'talks, Planets, TEDx, bacteria, biology, engineering, environment, evolution, exploration, future, innovation, intelligence, microbiology, nature, potential, science',\n",
       " 'talks, Debate, Guns, activism, big problems, children, choice, community, future, goal-setting, government, law, leadership, marketing, parenting, policy, social change, violence',\n",
       " 'talks, Brazil, Slavery, art, beauty, community, creativity, culture, design, global issues, humanity, identity, photography, race, social change, society, visualizations',\n",
       " 'talks, NASA, communication, computers, creativity, design, engineering, exploration, future, innovation, interface design, invention, microsoft, potential, prediction, product design, technology, visualizations',\n",
       " 'talks, Africa, Internet, community, democracy, development, future, government, identity, leadership, politics, potential',\n",
       " 'talks, ancient world, animals, biology, biosphere, curiosity, environment, evolution, history, life, nature, paleontology, science',\n",
       " 'talks, TEDx, business, culture, economics, personal growth, success, work',\n",
       " 'talks, Criminal Justice, big problems, choice, compassion, decision-making, education, government, inequality, law, leadership, policy, race, social change, society',\n",
       " 'talks, Senses, augmented reality, brain, computers, creativity, cyborg, demo, design, engineering, entrepreneur, innovation, interface design, invention, neuroscience, potential, prediction, product design, technology, visualizations']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_extrat(labels, valid_list):\n",
    "    extract_words = [keyword_processor.extract_keywords(item) for item in labels]\n",
    "    res = []\n",
    "    for item in extract_words:\n",
    "        if len(item) == 3:\n",
    "            res.append('TED')\n",
    "        elif len(item) == 2:\n",
    "            if 'technology' in item:\n",
    "                if 'entertainment' in item:\n",
    "                    res.append('TEo')\n",
    "                elif 'design' in item:\n",
    "                    res.append('ToD')\n",
    "            else:\n",
    "                res.append('oED')\n",
    "        elif len(item) == 1:\n",
    "            if item[0] == 'technology':\n",
    "                res.append('Too')\n",
    "            elif item[0] == 'entertainment':\n",
    "                res.append('oEo')\n",
    "            elif item[0] == 'design':\n",
    "                res.append('ooD')\n",
    "        else:\n",
    "            res.append('ooo')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_labels = label_extrat(labels, valid_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来看看具体每个标签的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "label_cnt = Counter()\n",
    "for item in new_labels:\n",
    "    label_cnt[item] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'TED': 55,\n",
       "         'TEo': 36,\n",
       "         'ToD': 122,\n",
       "         'Too': 381,\n",
       "         'oED': 38,\n",
       "         'oEo': 173,\n",
       "         'ooD': 147,\n",
       "         'ooo': 1133})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有的文本出现了类似“(Video) Hyowon Gweon: See this? ”的文本，我们需要去除括号内的内容和说话人的名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 快速清理文本利器，正则表达式！\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_preprocess(text):\n",
    "    res = []\n",
    "    for item in text:\n",
    "        item = re.sub(r'\\([^)]*\\)', '', item) #去除括号内的内容\n",
    "        # 去除说话人的名字\n",
    "        sentences = []\n",
    "        for line in item.split('\\n'):\n",
    "            m = re.match(r'^(?:(?P<precolon>[^:]{,20}):)?(?P<postcolon>.*)$', line)\n",
    "            sentences.extend(sent for sent in re.split('\\.|!|\\?', m.groupdict()['postcolon']) if sent)\n",
    "        item = '.'.join(sentences)\n",
    "        item = re.sub(r'[^a-z0-9]+', ' ', item.lower()) #只保留小写字母和数字\n",
    "        res.append(item.split())\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们想要保留句号等表示句子结束的符号，来作为词向量训练的语料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedding_text_process(text):\n",
    "    res = []\n",
    "    for item in text:\n",
    "        # 去除说话人的名字\n",
    "        sentences = []\n",
    "        for line in item.split('\\n'):\n",
    "            m = re.match(r'^(?:(?P<precolon>[^:]{,20}):)?(?P<postcolon>.*)$', line)\n",
    "            sentences.extend(sent for sent in re.split('\\.|!|\\?', m.groupdict()['postcolon']) if sent)\n",
    "        item = '.'.join(sentences)\n",
    "        item = re.sub(r'\\([^)]*\\)', '', item) #去除括号内的内容\n",
    "        item = re.sub(' \\.{3,} ', ' ', item) #去除省略号\n",
    "        item = re.split('\\.|!|\\?',item) #以句号、感叹号和问号作为断句点\n",
    "        item = [row.strip() for row in item if row] #去除多余的空格和空行\n",
    "        item = [re.sub(r'[^a-z0-9]+', ' ', row.lower()).split() for row in item]  #只保留小写字母和数字\n",
    "        for row in item:\n",
    "            if row != []:\n",
    "                res.append(row)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['here',\n",
       " 'are',\n",
       " 'two',\n",
       " 'reasons',\n",
       " 'companies',\n",
       " 'fail',\n",
       " 'they',\n",
       " 'only',\n",
       " 'do',\n",
       " 'more',\n",
       " 'of',\n",
       " 'the',\n",
       " 'same',\n",
       " 'or',\n",
       " 'they',\n",
       " 'only',\n",
       " 'do',\n",
       " 'what',\n",
       " 's',\n",
       " 'new']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = embedding_text_process(input_text)\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['here',\n",
       " 'are',\n",
       " 'two',\n",
       " 'reasons',\n",
       " 'companies',\n",
       " 'fail',\n",
       " 'they',\n",
       " 'only',\n",
       " 'do',\n",
       " 'more',\n",
       " 'of',\n",
       " 'the',\n",
       " 'same',\n",
       " 'or',\n",
       " 'they',\n",
       " 'only',\n",
       " 'do',\n",
       " 'what',\n",
       " 's',\n",
       " 'new',\n",
       " 'to',\n",
       " 'me',\n",
       " 'the',\n",
       " 'real',\n",
       " 'real',\n",
       " 'solution',\n",
       " 'to',\n",
       " 'quality',\n",
       " 'growth',\n",
       " 'is',\n",
       " 'figuring',\n",
       " 'out',\n",
       " 'the',\n",
       " 'balance',\n",
       " 'between',\n",
       " 'two',\n",
       " 'activities',\n",
       " 'exploration',\n",
       " 'and',\n",
       " 'exploitation',\n",
       " 'both',\n",
       " 'are',\n",
       " 'necessary',\n",
       " 'but',\n",
       " 'it',\n",
       " 'can',\n",
       " 'be',\n",
       " 'too',\n",
       " 'much',\n",
       " 'of',\n",
       " 'a',\n",
       " 'good',\n",
       " 'thing',\n",
       " 'consider',\n",
       " 'facit',\n",
       " 'i',\n",
       " 'm',\n",
       " 'actually',\n",
       " 'old',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'remember',\n",
       " 'them',\n",
       " 'facit',\n",
       " 'was',\n",
       " 'a',\n",
       " 'fantastic',\n",
       " 'company',\n",
       " 'they',\n",
       " 'were',\n",
       " 'born',\n",
       " 'deep',\n",
       " 'in',\n",
       " 'the',\n",
       " 'swedish',\n",
       " 'forest',\n",
       " 'and',\n",
       " 'they',\n",
       " 'made',\n",
       " 'the',\n",
       " 'best',\n",
       " 'mechanical',\n",
       " 'calculators',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " 'everybody',\n",
       " 'used',\n",
       " 'them',\n",
       " 'and',\n",
       " 'what',\n",
       " 'did',\n",
       " 'facit',\n",
       " 'do',\n",
       " 'when',\n",
       " 'the',\n",
       " 'electronic',\n",
       " 'calculator',\n",
       " 'came',\n",
       " 'along',\n",
       " 'they',\n",
       " 'continued',\n",
       " 'doing',\n",
       " 'exactly',\n",
       " 'the',\n",
       " 'same',\n",
       " 'in',\n",
       " 'six',\n",
       " 'months',\n",
       " 'they',\n",
       " 'went',\n",
       " 'from',\n",
       " 'maximum',\n",
       " 'revenue',\n",
       " 'and',\n",
       " 'they',\n",
       " 'were',\n",
       " 'gone',\n",
       " 'gone',\n",
       " 'to',\n",
       " 'me',\n",
       " 'the',\n",
       " 'irony',\n",
       " 'about',\n",
       " 'the',\n",
       " 'facit',\n",
       " 'story',\n",
       " 'is',\n",
       " 'hearing',\n",
       " 'about',\n",
       " 'the',\n",
       " 'facit',\n",
       " 'engineers',\n",
       " 'who',\n",
       " 'had',\n",
       " 'bought',\n",
       " 'cheap',\n",
       " 'small',\n",
       " 'electronic',\n",
       " 'calculators',\n",
       " 'in',\n",
       " 'japan',\n",
       " 'that',\n",
       " 'they',\n",
       " 'used',\n",
       " 'to',\n",
       " 'double',\n",
       " 'check',\n",
       " 'their',\n",
       " 'calculators',\n",
       " 'facit',\n",
       " 'did',\n",
       " 'too',\n",
       " 'much',\n",
       " 'exploitation',\n",
       " 'but',\n",
       " 'exploration',\n",
       " 'can',\n",
       " 'go',\n",
       " 'wild',\n",
       " 'too',\n",
       " 'a',\n",
       " 'few',\n",
       " 'years',\n",
       " 'back',\n",
       " 'i',\n",
       " 'worked',\n",
       " 'closely',\n",
       " 'alongside',\n",
       " 'a',\n",
       " 'european',\n",
       " 'biotech',\n",
       " 'company',\n",
       " 'let',\n",
       " 's',\n",
       " 'call',\n",
       " 'them',\n",
       " 'oncosearch',\n",
       " 'the',\n",
       " 'company',\n",
       " 'was',\n",
       " 'brilliant',\n",
       " 'they',\n",
       " 'had',\n",
       " 'applications',\n",
       " 'that',\n",
       " 'promised',\n",
       " 'to',\n",
       " 'diagnose',\n",
       " 'even',\n",
       " 'cure',\n",
       " 'certain',\n",
       " 'forms',\n",
       " 'of',\n",
       " 'blood',\n",
       " 'cancer',\n",
       " 'every',\n",
       " 'day',\n",
       " 'was',\n",
       " 'about',\n",
       " 'creating',\n",
       " 'something',\n",
       " 'new',\n",
       " 'they',\n",
       " 'were',\n",
       " 'extremely',\n",
       " 'innovative',\n",
       " 'and',\n",
       " 'the',\n",
       " 'mantra',\n",
       " 'was',\n",
       " 'when',\n",
       " 'we',\n",
       " 'only',\n",
       " 'get',\n",
       " 'it',\n",
       " 'right',\n",
       " 'or',\n",
       " 'even',\n",
       " 'we',\n",
       " 'want',\n",
       " 'it',\n",
       " 'perfect',\n",
       " 'the',\n",
       " 'sad',\n",
       " 'thing',\n",
       " 'is',\n",
       " 'before',\n",
       " 'they',\n",
       " 'became',\n",
       " 'perfect',\n",
       " 'even',\n",
       " 'good',\n",
       " 'enough',\n",
       " 'they',\n",
       " 'became',\n",
       " 'obsolete',\n",
       " 'oncosearch',\n",
       " 'did',\n",
       " 'too',\n",
       " 'much',\n",
       " 'exploration',\n",
       " 'i',\n",
       " 'first',\n",
       " 'heard',\n",
       " 'about',\n",
       " 'exploration',\n",
       " 'and',\n",
       " 'exploitation',\n",
       " 'about',\n",
       " '15',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'when',\n",
       " 'i',\n",
       " 'worked',\n",
       " 'as',\n",
       " 'a',\n",
       " 'visiting',\n",
       " 'scholar',\n",
       " 'at',\n",
       " 'stanford',\n",
       " 'university',\n",
       " 'the',\n",
       " 'founder',\n",
       " 'of',\n",
       " 'the',\n",
       " 'idea',\n",
       " 'is',\n",
       " 'jim',\n",
       " 'march',\n",
       " 'and',\n",
       " 'to',\n",
       " 'me',\n",
       " 'the',\n",
       " 'power',\n",
       " 'of',\n",
       " 'the',\n",
       " 'idea',\n",
       " 'is',\n",
       " 'its',\n",
       " 'practicality',\n",
       " 'exploration',\n",
       " 'exploration',\n",
       " 'is',\n",
       " 'about',\n",
       " 'coming',\n",
       " 'up',\n",
       " 'with',\n",
       " 'what',\n",
       " 's',\n",
       " 'new',\n",
       " 'it',\n",
       " 's',\n",
       " 'about',\n",
       " 'search',\n",
       " 'it',\n",
       " 's',\n",
       " 'about',\n",
       " 'discovery',\n",
       " 'it',\n",
       " 's',\n",
       " 'about',\n",
       " 'new',\n",
       " 'products',\n",
       " 'it',\n",
       " 's',\n",
       " 'about',\n",
       " 'new',\n",
       " 'innovations',\n",
       " 'it',\n",
       " 's',\n",
       " 'about',\n",
       " 'changing',\n",
       " 'our',\n",
       " 'frontiers',\n",
       " 'our',\n",
       " 'heroes',\n",
       " 'are',\n",
       " 'people',\n",
       " 'who',\n",
       " 'have',\n",
       " 'done',\n",
       " 'exploration',\n",
       " 'madame',\n",
       " 'curie',\n",
       " 'picasso',\n",
       " 'neil',\n",
       " 'armstrong',\n",
       " 'sir',\n",
       " 'edmund',\n",
       " 'hillary',\n",
       " 'etc',\n",
       " 'i',\n",
       " 'come',\n",
       " 'from',\n",
       " 'norway',\n",
       " 'all',\n",
       " 'our',\n",
       " 'heroes',\n",
       " 'are',\n",
       " 'explorers',\n",
       " 'and',\n",
       " 'they',\n",
       " 'deserve',\n",
       " 'to',\n",
       " 'be',\n",
       " 'we',\n",
       " 'all',\n",
       " 'know',\n",
       " 'that',\n",
       " 'exploration',\n",
       " 'is',\n",
       " 'risky',\n",
       " 'we',\n",
       " 'don',\n",
       " 't',\n",
       " 'know',\n",
       " 'the',\n",
       " 'answers',\n",
       " 'we',\n",
       " 'don',\n",
       " 't',\n",
       " 'know',\n",
       " 'if',\n",
       " 'we',\n",
       " 're',\n",
       " 'going',\n",
       " 'to',\n",
       " 'find',\n",
       " 'them',\n",
       " 'and',\n",
       " 'we',\n",
       " 'know',\n",
       " 'that',\n",
       " 'the',\n",
       " 'risks',\n",
       " 'are',\n",
       " 'high',\n",
       " 'exploitation',\n",
       " 'is',\n",
       " 'the',\n",
       " 'opposite',\n",
       " 'exploitation',\n",
       " 'is',\n",
       " 'taking',\n",
       " 'the',\n",
       " 'knowledge',\n",
       " 'we',\n",
       " 'have',\n",
       " 'and',\n",
       " 'making',\n",
       " 'good',\n",
       " 'better',\n",
       " 'exploitation',\n",
       " 'is',\n",
       " 'about',\n",
       " 'making',\n",
       " 'our',\n",
       " 'trains',\n",
       " 'run',\n",
       " 'on',\n",
       " 'time',\n",
       " 'it',\n",
       " 's',\n",
       " 'about',\n",
       " 'making',\n",
       " 'good',\n",
       " 'products',\n",
       " 'faster',\n",
       " 'and',\n",
       " 'cheaper',\n",
       " 'exploitation',\n",
       " 'is',\n",
       " 'not',\n",
       " 'risky',\n",
       " 'in',\n",
       " 'the',\n",
       " 'short',\n",
       " 'term',\n",
       " 'but',\n",
       " 'if',\n",
       " 'we',\n",
       " 'only',\n",
       " 'exploit',\n",
       " 'it',\n",
       " 's',\n",
       " 'very',\n",
       " 'risky',\n",
       " 'in',\n",
       " 'the',\n",
       " 'long',\n",
       " 'term',\n",
       " 'and',\n",
       " 'i',\n",
       " 'think',\n",
       " 'we',\n",
       " 'all',\n",
       " 'have',\n",
       " 'memories',\n",
       " 'of',\n",
       " 'the',\n",
       " 'famous',\n",
       " 'pop',\n",
       " 'groups',\n",
       " 'who',\n",
       " 'keep',\n",
       " 'singing',\n",
       " 'the',\n",
       " 'same',\n",
       " 'songs',\n",
       " 'again',\n",
       " 'and',\n",
       " 'again',\n",
       " 'until',\n",
       " 'they',\n",
       " 'become',\n",
       " 'obsolete',\n",
       " 'or',\n",
       " 'even',\n",
       " 'pathetic',\n",
       " 'that',\n",
       " 's',\n",
       " 'the',\n",
       " 'risk',\n",
       " 'of',\n",
       " 'exploitation',\n",
       " 'so',\n",
       " 'if',\n",
       " 'we',\n",
       " 'take',\n",
       " 'a',\n",
       " 'long',\n",
       " 'term',\n",
       " 'perspective',\n",
       " 'we',\n",
       " 'explore',\n",
       " 'if',\n",
       " 'we',\n",
       " 'take',\n",
       " 'a',\n",
       " 'short',\n",
       " 'term',\n",
       " 'perspective',\n",
       " 'we',\n",
       " 'exploit',\n",
       " 'small',\n",
       " 'children',\n",
       " 'they',\n",
       " 'explore',\n",
       " 'all',\n",
       " 'day',\n",
       " 'all',\n",
       " 'day',\n",
       " 'it',\n",
       " 's',\n",
       " 'about',\n",
       " 'exploration',\n",
       " 'as',\n",
       " 'we',\n",
       " 'grow',\n",
       " 'older',\n",
       " 'we',\n",
       " 'explore',\n",
       " 'less',\n",
       " 'because',\n",
       " 'we',\n",
       " 'have',\n",
       " 'more',\n",
       " 'knowledge',\n",
       " 'to',\n",
       " 'exploit',\n",
       " 'on',\n",
       " 'the',\n",
       " 'same',\n",
       " 'goes',\n",
       " 'for',\n",
       " 'companies',\n",
       " 'companies',\n",
       " 'become',\n",
       " 'by',\n",
       " 'nature',\n",
       " 'less',\n",
       " 'innovative',\n",
       " 'as',\n",
       " 'they',\n",
       " 'become',\n",
       " 'more',\n",
       " 'competent',\n",
       " 'and',\n",
       " 'this',\n",
       " 'is',\n",
       " 'of',\n",
       " 'course',\n",
       " 'a',\n",
       " 'big',\n",
       " 'worry',\n",
       " 'to',\n",
       " 'ceos',\n",
       " 'and',\n",
       " 'i',\n",
       " 'hear',\n",
       " 'very',\n",
       " 'often',\n",
       " 'questions',\n",
       " 'phrased',\n",
       " 'in',\n",
       " 'different',\n",
       " 'ways',\n",
       " 'for',\n",
       " 'example',\n",
       " 'how',\n",
       " 'can',\n",
       " 'i',\n",
       " 'both',\n",
       " 'effectively',\n",
       " 'run',\n",
       " 'and',\n",
       " 'reinvent',\n",
       " 'my',\n",
       " 'company',\n",
       " 'or',\n",
       " 'how',\n",
       " 'can',\n",
       " 'i',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'that',\n",
       " 'our',\n",
       " 'company',\n",
       " 'changes',\n",
       " 'before',\n",
       " 'we',\n",
       " 'become',\n",
       " 'obsolete',\n",
       " 'or',\n",
       " 'are',\n",
       " 'hit',\n",
       " 'by',\n",
       " 'a',\n",
       " 'crisis',\n",
       " 'so',\n",
       " 'doing',\n",
       " 'one',\n",
       " 'well',\n",
       " 'is',\n",
       " 'difficult',\n",
       " 'doing',\n",
       " 'both',\n",
       " 'well',\n",
       " 'as',\n",
       " 'the',\n",
       " 'same',\n",
       " 'time',\n",
       " 'is',\n",
       " 'art',\n",
       " 'pushing',\n",
       " 'both',\n",
       " 'exploration',\n",
       " 'and',\n",
       " 'exploitation',\n",
       " 'so',\n",
       " 'one',\n",
       " 'thing',\n",
       " 'we',\n",
       " 've',\n",
       " 'found',\n",
       " 'is',\n",
       " 'only',\n",
       " 'about',\n",
       " 'two',\n",
       " 'percent',\n",
       " 'of',\n",
       " 'companies',\n",
       " 'are',\n",
       " 'able',\n",
       " 'to',\n",
       " 'effectively',\n",
       " 'explore',\n",
       " 'and',\n",
       " 'exploit',\n",
       " 'at',\n",
       " 'the',\n",
       " 'same',\n",
       " 'time',\n",
       " 'in',\n",
       " 'parallel',\n",
       " 'but',\n",
       " 'when',\n",
       " 'they',\n",
       " 'do',\n",
       " 'the',\n",
       " 'payoffs',\n",
       " 'are',\n",
       " 'huge',\n",
       " 'so',\n",
       " 'we',\n",
       " 'have',\n",
       " 'lots',\n",
       " 'of',\n",
       " 'great',\n",
       " 'examples',\n",
       " 'we',\n",
       " 'have',\n",
       " 'nestl',\n",
       " 'creating',\n",
       " 'nespresso',\n",
       " 'we',\n",
       " 'have',\n",
       " 'lego',\n",
       " 'going',\n",
       " 'into',\n",
       " 'animated',\n",
       " 'films',\n",
       " 'toyota',\n",
       " 'creating',\n",
       " 'the',\n",
       " 'hybrids',\n",
       " 'unilever',\n",
       " 'pushing',\n",
       " 'into',\n",
       " 'sustainability',\n",
       " 'there',\n",
       " 'are',\n",
       " 'lots',\n",
       " 'of',\n",
       " 'examples',\n",
       " 'and',\n",
       " 'the',\n",
       " 'benefits',\n",
       " 'are',\n",
       " 'huge',\n",
       " 'why',\n",
       " 'is',\n",
       " 'balancing',\n",
       " 'so',\n",
       " 'difficult',\n",
       " 'i',\n",
       " 'think',\n",
       " 'it',\n",
       " 's',\n",
       " 'difficult',\n",
       " 'because',\n",
       " 'there',\n",
       " 'are',\n",
       " 'so',\n",
       " 'many',\n",
       " 'traps',\n",
       " 'that',\n",
       " 'keep',\n",
       " 'us',\n",
       " 'where',\n",
       " 'we',\n",
       " 'are',\n",
       " 'so',\n",
       " 'i',\n",
       " 'll',\n",
       " 'talk',\n",
       " 'about',\n",
       " 'two',\n",
       " 'but',\n",
       " 'there',\n",
       " 'are',\n",
       " 'many',\n",
       " 'so',\n",
       " 'let',\n",
       " 's',\n",
       " 'talk',\n",
       " 'about',\n",
       " 'the',\n",
       " 'perpetual',\n",
       " 'search',\n",
       " 'trap',\n",
       " 'we',\n",
       " 'discover',\n",
       " 'something',\n",
       " 'but',\n",
       " 'we',\n",
       " 'don',\n",
       " 't',\n",
       " 'have',\n",
       " 'the',\n",
       " 'patience',\n",
       " 'or',\n",
       " 'the',\n",
       " 'persistence',\n",
       " 'to',\n",
       " 'get',\n",
       " 'at',\n",
       " 'it',\n",
       " 'and',\n",
       " 'make',\n",
       " 'it',\n",
       " 'work',\n",
       " 'so',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'staying',\n",
       " 'with',\n",
       " 'it',\n",
       " 'we',\n",
       " 'create',\n",
       " 'something',\n",
       " 'new',\n",
       " 'but',\n",
       " 'the',\n",
       " 'same',\n",
       " 'goes',\n",
       " 'for',\n",
       " 'that',\n",
       " 'then',\n",
       " 'we',\n",
       " 're',\n",
       " 'in',\n",
       " 'the',\n",
       " 'vicious',\n",
       " 'circle',\n",
       " 'of',\n",
       " 'actually',\n",
       " 'coming',\n",
       " 'up',\n",
       " 'with',\n",
       " 'ideas',\n",
       " 'but',\n",
       " 'being',\n",
       " 'frustrated',\n",
       " 'oncosearch',\n",
       " 'was',\n",
       " 'a',\n",
       " 'good',\n",
       " 'example',\n",
       " 'a',\n",
       " 'famous',\n",
       " 'example',\n",
       " 'is',\n",
       " 'of',\n",
       " 'course',\n",
       " 'xerox',\n",
       " 'but',\n",
       " 'we',\n",
       " 'don',\n",
       " 't',\n",
       " 'only',\n",
       " 'see',\n",
       " 'this',\n",
       " 'in',\n",
       " 'companies',\n",
       " 'we',\n",
       " 'see',\n",
       " 'this',\n",
       " 'in',\n",
       " 'the',\n",
       " 'public',\n",
       " 'sector',\n",
       " 'as',\n",
       " 'well',\n",
       " 'we',\n",
       " 'all',\n",
       " 'know',\n",
       " 'that',\n",
       " 'any',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'effective',\n",
       " 'reform',\n",
       " 'of',\n",
       " 'education',\n",
       " 'research',\n",
       " 'health',\n",
       " 'care',\n",
       " 'even',\n",
       " 'defense',\n",
       " 'takes',\n",
       " '10',\n",
       " '15',\n",
       " 'maybe',\n",
       " '20',\n",
       " 'years',\n",
       " 'to',\n",
       " 'work',\n",
       " 'but',\n",
       " 'still',\n",
       " 'we',\n",
       " 'change',\n",
       " 'much',\n",
       " 'more',\n",
       " 'often',\n",
       " 'we',\n",
       " 'really',\n",
       " 'don',\n",
       " 't',\n",
       " 'give',\n",
       " 'them',\n",
       " 'the',\n",
       " 'chance',\n",
       " 'another',\n",
       " 'trap',\n",
       " 'is',\n",
       " 'the',\n",
       " 'success',\n",
       " 'trap',\n",
       " 'facit',\n",
       " 'fell',\n",
       " 'into',\n",
       " 'the',\n",
       " 'success',\n",
       " 'trap',\n",
       " 'they',\n",
       " 'literally',\n",
       " 'held',\n",
       " 'the',\n",
       " 'future',\n",
       " 'in',\n",
       " 'their',\n",
       " 'hands',\n",
       " 'but',\n",
       " 'they',\n",
       " 'couldn',\n",
       " 't',\n",
       " 'see',\n",
       " 'it',\n",
       " 'they',\n",
       " 'were',\n",
       " 'simply',\n",
       " 'so',\n",
       " 'good',\n",
       " 'at',\n",
       " 'making',\n",
       " 'what',\n",
       " 'they',\n",
       " 'loved',\n",
       " 'doing',\n",
       " 'that',\n",
       " 'they',\n",
       " 'wouldn',\n",
       " 't',\n",
       " 'change',\n",
       " 'we',\n",
       " 'are',\n",
       " 'like',\n",
       " 'that',\n",
       " 'too',\n",
       " 'when',\n",
       " 'we',\n",
       " 'know',\n",
       " 'something',\n",
       " 'well',\n",
       " 'it',\n",
       " 's',\n",
       " 'difficult',\n",
       " 'to',\n",
       " 'change',\n",
       " 'bill',\n",
       " 'gates',\n",
       " 'has',\n",
       " 'said',\n",
       " 'success',\n",
       " 'is',\n",
       " 'a',\n",
       " 'lousy',\n",
       " 'teacher',\n",
       " 'it',\n",
       " 'seduces',\n",
       " 'us',\n",
       " 'into',\n",
       " 'thinking',\n",
       " 'we',\n",
       " 'cannot',\n",
       " 'fail',\n",
       " 'that',\n",
       " 's',\n",
       " 'the',\n",
       " 'challenge',\n",
       " 'with',\n",
       " 'success',\n",
       " 'so',\n",
       " 'i',\n",
       " 'think',\n",
       " 'there',\n",
       " 'are',\n",
       " 'some',\n",
       " 'lessons',\n",
       " 'and',\n",
       " 'i',\n",
       " 'think',\n",
       " 'they',\n",
       " 'apply',\n",
       " 'to',\n",
       " 'us',\n",
       " 'and',\n",
       " 'they',\n",
       " 'apply',\n",
       " 'to',\n",
       " 'our',\n",
       " 'companies',\n",
       " 'the',\n",
       " 'first',\n",
       " 'lesson',\n",
       " 'is',\n",
       " 'get',\n",
       " 'ahead',\n",
       " 'of',\n",
       " 'the',\n",
       " 'crisis',\n",
       " 'and',\n",
       " 'any',\n",
       " 'company',\n",
       " 'that',\n",
       " 's',\n",
       " 'able',\n",
       " 'to',\n",
       " 'innovate',\n",
       " 'is',\n",
       " 'actually',\n",
       " 'able',\n",
       " 'to',\n",
       " 'also',\n",
       " 'buy',\n",
       " 'an',\n",
       " 'insurance',\n",
       " 'in',\n",
       " 'the',\n",
       " 'future',\n",
       " 'netflix',\n",
       " 'they',\n",
       " 'could',\n",
       " 'so',\n",
       " 'easily',\n",
       " 'have',\n",
       " 'been',\n",
       " 'content',\n",
       " 'with',\n",
       " 'earlier',\n",
       " 'generations',\n",
       " 'of',\n",
       " 'distribution',\n",
       " 'but',\n",
       " 'they',\n",
       " 'always',\n",
       " 'and',\n",
       " 'i',\n",
       " 'think',\n",
       " 'they',\n",
       " 'will',\n",
       " 'always',\n",
       " 'keep',\n",
       " 'pushing',\n",
       " 'for',\n",
       " 'the',\n",
       " 'next',\n",
       " 'battle',\n",
       " 'i',\n",
       " 'see',\n",
       " 'other',\n",
       " 'companies',\n",
       " 'that',\n",
       " 'say',\n",
       " 'i',\n",
       " 'll',\n",
       " 'win',\n",
       " ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text = text_preprocess(input_text)\n",
    "new_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文档的长度直方图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20656d6ada0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "len_doc = [len(x) for x in new_text]\n",
    "Y_plot, X_plot = np.histogram(len_doc, bins=10)\n",
    "X_plot = np.arange(10)\n",
    "plt.bar(X_plot, Y_plot, facecolor='#9999ff', edgecolor='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词向量训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里会使用三种词向量，分别是：\n",
    "* 使用当前语料自己训练一个词向量\n",
    "* 使用glove词向量：基于词共现矩阵训练的词向量 https://nlp.stanford.edu/projects/glove/\n",
    "* 使用fastext词向量：基于预测方法训练的词向量，word2vec的改进版 https://fasttext.cc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "软件包版本：\n",
    "* Gensim 3.2\n",
    "* mxnet-cu90 1.0.1 master\n",
    "\n",
    "通过mxnet的text API轻松调用glove和fasttext的预训练词向量\n",
    "\n",
    "更新mxnet到master版本：pip install mxnet --pre --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from mxnet.contrib import text\n",
    "from mxnet import gluon\n",
    "from mxnet import nd\n",
    "from mxnet.gluon import nn, rnn\n",
    "import mxnet as mx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 使用gensim训练词向量\n",
    "def word2vec(text, fname, ndims, window_size, min_cnt = 1):\n",
    "    model = Word2Vec(text, min_count = min_cnt, window = window_size, size = ndims)\n",
    "    word_vectors = model.wv\n",
    "    word_vectors.save_word2vec_format(fname, binary = False)\n",
    "    return model\n",
    "\n",
    "# 从本地文件加载词向量\n",
    "def load_embeddings(fname):\n",
    "    embeddings_index = {}\n",
    "    f = open(fname, encoding = 'utf-8')\n",
    "    for k,line in enumerate(f.readlines()):\n",
    "        if k != 0:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#w2v_model = word2vec(sentences, 'gensim_w2v_300d.txt', 300, 5, 5)\n",
    "w2v_embeddings = load_embeddings('gensim_w2v_300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21448"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mxnet Text API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<input>:10: DeprecationWarning: invalid escape sequence \\.\n",
      "<input>:10: DeprecationWarning: invalid escape sequence \\.\n",
      "<input>:10: DeprecationWarning: invalid escape sequence \\.\n",
      "<input>:10: DeprecationWarning: invalid escape sequence \\.\n",
      "<input>:10: DeprecationWarning: invalid escape sequence \\.\n",
      "<input>:10: DeprecationWarning: invalid escape sequence \\.\n",
      "<input>:10: DeprecationWarning: invalid escape sequence \\.\n",
      "<input>:10: DeprecationWarning: invalid escape sequence \\.\n",
      "<input>:10: DeprecationWarning: invalid escape sequence \\.\n",
      "<input>:10: DeprecationWarning: invalid escape sequence \\.\n",
      "<input>:10: DeprecationWarning: invalid escape sequence \\.\n",
      "<input>:10: DeprecationWarning: invalid escape sequence \\.\n",
      "<input>:10: DeprecationWarning: invalid escape sequence \\.\n",
      "<input>:10: DeprecationWarning: invalid escape sequence \\.\n",
      "<input>:10: DeprecationWarning: invalid escape sequence \\.\n",
      "<input>:10: DeprecationWarning: invalid escape sequence \\.\n",
      "<input>:10: DeprecationWarning: invalid escape sequence \\.\n",
      "<input>:10: DeprecationWarning: invalid escape sequence \\.\n",
      "<input>:10: DeprecationWarning: invalid escape sequence \\.\n",
      "<input>:10: DeprecationWarning: invalid escape sequence \\.\n",
      "<input>:10: DeprecationWarning: invalid escape sequence \\.\n",
      "<input>:10: DeprecationWarning: invalid escape sequence \\.\n",
      "<input>:10: DeprecationWarning: invalid escape sequence \\.\n",
      "<input>:10: DeprecationWarning: invalid escape sequence \\.\n",
      "<ipython-input-15-470c806243c2>:10: DeprecationWarning: invalid escape sequence \\.\n",
      "  sentences.extend(sent for sent in re.split('\\.|!|\\?', m.groupdict()['postcolon']) if sent)\n"
     ]
    }
   ],
   "source": [
    "# 目前只能从字符串文本中构建词向量\n",
    "def text_process_str(text):\n",
    "    res = []\n",
    "    for item in text:\n",
    "        item = re.sub(r'\\([^)]*\\)', '', item) #去除括号内的内容\n",
    "        # 去除说话人的名字\n",
    "        sentences = []\n",
    "        for line in item.split('\\n'):\n",
    "            m = re.match(r'^(?:(?P<precolon>[^:]{,20}):)?(?P<postcolon>.*)$', line)\n",
    "            sentences.extend(sent for sent in re.split('\\.|!|\\?', m.groupdict()['postcolon']) if sent)\n",
    "        item = '.'.join(sentences)\n",
    "        item = re.sub(r'[^a-z0-9]+', ' ', item.lower()) #只保留小写字母和数字\n",
    "        res.append(item)\n",
    "    final_res = '\\n'.join(res)\n",
    "    return final_res\n",
    "\n",
    "text_str = text_process_str(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter = text.utils.count_tokens_from_str(text_str)\n",
    "my_vocab = text.vocab.Vocabulary(counter, most_freq_count=21448, unknown_token='<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\mxnet\\contrib\\text\\embedding.py:278: UserWarning: At line 1 of the pre-trained text embedding file: token 111051 with 1-dimensional vector [300.0] is likely a header and is skipped.\n",
      "  'skipped.' % (line_num, token, elems))\n"
     ]
    }
   ],
   "source": [
    "fasttext_embedding = text.embedding.create('fasttext', pretrained_file_name='wiki.simple.vec', vocabulary=my_vocab)\n",
    "glove_embedding = text.embedding.create('glove', pretrained_file_name='glove.6B.300d.txt', vocabulary=my_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算词向量相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm_vecs_by_row(x):\n",
    "    return x / nd.sqrt(nd.sum(x * x, axis=1)).reshape((-1,1))\n",
    "\n",
    "def get_knn(token_embedding, k, word):\n",
    "    word_vec = token_embedding.get_vecs_by_tokens([word]).reshape((-1, 1))\n",
    "    vocab_vecs = norm_vecs_by_row(token_embedding.idx_to_vec)\n",
    "    dot_prod = nd.dot(vocab_vecs, word_vec)\n",
    "    indices = nd.topk(dot_prod.reshape((len(token_embedding), )), k=k+2,\n",
    "                      ret_typ='indices')\n",
    "    indices = [int(i.asscalar()) for i in indices]\n",
    "    # 除去未知词符号和输入词。\n",
    "    return token_embedding.to_tokens(indices[2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到glove和fasttext基于更大的的语料训练得到的词的相似性更加准确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w2v_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-620d4007f710>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mw2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'apple'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'w2v_model' is not defined"
     ]
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iphone', 'ipod', 'microsoft', 'ipad', 'intel']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_knn(glove_embedding, 5, 'apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apples', 'app', 'iphone', 'ipod', 'ipad']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_knn(fasttext_embedding, 5, 'apple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来将从简单的模型开始，模型依次为：\n",
    "* 单层感知机\n",
    "* LSTM\n",
    "* GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn, rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN模型初始化定义，包含四个模型，分别是：\n",
    "* rnn_relu\n",
    "* rnn_tanh\n",
    "* rnn_sigmoid\n",
    "* lstm\n",
    "* gru\n",
    "* bi-gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNModel(gluon.Block):\n",
    "    \"\"\"循环神经网络模型库\"\"\"\n",
    "    def __init__(self, mode, embed_matrix, vocab_size, embed_dim, hidden_dim,\n",
    "                 num_layers, num_labels, dropout=0.5, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.drop = nn.Dropout(dropout)\n",
    "            if embed_matrix:\n",
    "                self.encoder = nn.Embedding(vocab_size, embed_dim)\n",
    "                #self.encoder.initialize()\n",
    "                self.encoder.weight.set_data(embed_matrix.idx_to_vec)\n",
    "            else:\n",
    "                self.encoder = nn.Embedding(vocab_size, embed_dim)\n",
    "            \n",
    "            if mode == 'rnn_relu':\n",
    "                self.rnn = rnn.RNN(hidden_dim, num_layers, activation='relu',\n",
    "                                   dropout=dropout, input_size=embed_dim)\n",
    "            elif mode == 'rnn_sigmoid':\n",
    "                self.rnn = rnn.RNN(hidden_dim, num_layers, activation='sigmoid',\n",
    "                                   dropout=dropout, input_size=embed_dim)    \n",
    "            elif mode == 'rnn_tanh':\n",
    "                self.rnn = rnn.RNN(hidden_dim, num_layers, dropout=dropout,\n",
    "                                   input_size=embed_dim)\n",
    "            elif mode == 'lstm':\n",
    "                self.rnn = rnn.LSTM(hidden_dim, num_layers, dropout=dropout,\n",
    "                                    input_size=embed_dim)\n",
    "            elif mode == 'gru':\n",
    "                self.rnn = rnn.GRU(hidden_dim, num_layers, dropout=dropout,\n",
    "                                   input_size=embed_dim)\n",
    "            elif mode == 'bi-gru':\n",
    "                self.rnn = rnn.GRU(hidden_dim, num_layers, dropout=dropout,\n",
    "                                   bidirectional=True, input_size=embed_dim)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid mode %s. Options are rnn_relu, \"\n",
    "                                 \"rnn_tanh, lstm, and gru\"%mode)\n",
    "\n",
    "            self.decoder = nn.Dense(num_labels, in_units=hidden_dim, \n",
    "                                    activation='softmax')\n",
    "            self.hidden_dim = hidden_dim\n",
    "            \n",
    "    def forward(self, inputs, state):\n",
    "        emb = self.encoder(inputs)\n",
    "        output, state = self.rnn(emb, state)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.reshape((-1, self.hidden_dim)))\n",
    "        return decoded, state\n",
    "\n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = 'bi-gru'\n",
    "\n",
    "embed_dim = 300 # 词向量维度\n",
    "hidden_dim = 100 # RNN隐藏层大小\n",
    "num_layers = 2 # RNN隐藏层层数\n",
    "lr = 0.001 # 初始学习率\n",
    "lr_decay = 80 # 每xxx步学习率下降\n",
    "clipping_norm = 0.2 # 梯度裁剪大小\n",
    "epochs = 10 # 迭代轮数\n",
    "batch_size = 64 # 批量大小\n",
    "\n",
    "dropout_rate = 0.5 # 丢弃率\n",
    "eval_period = 100 # 验证步数\n",
    "num_labels = len(label_cnt) # 标签个数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = my_vocab.token_to_idx\n",
    "idx2vocab = {v:k for k,v in vocab.items()}\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 0,\n",
       " 'the': 1,\n",
       " 'and': 2,\n",
       " 'to': 3,\n",
       " 'of': 4,\n",
       " 'a': 5,\n",
       " 'that': 6,\n",
       " 'i': 7,\n",
       " 'in': 8,\n",
       " 'it': 9,\n",
       " 'you': 10,\n",
       " 'we': 11,\n",
       " 'is': 12,\n",
       " 's': 13,\n",
       " 'this': 14,\n",
       " 'so': 15,\n",
       " 'they': 16,\n",
       " 'was': 17,\n",
       " 'for': 18,\n",
       " 'are': 19,\n",
       " 'have': 20,\n",
       " 'but': 21,\n",
       " 'what': 22,\n",
       " 'on': 23,\n",
       " 'with': 24,\n",
       " 'can': 25,\n",
       " 't': 26,\n",
       " 'about': 27,\n",
       " 'there': 28,\n",
       " 'be': 29,\n",
       " 'as': 30,\n",
       " 'at': 31,\n",
       " 'all': 32,\n",
       " 'not': 33,\n",
       " 'do': 34,\n",
       " 'my': 35,\n",
       " 'one': 36,\n",
       " 're': 37,\n",
       " 'people': 38,\n",
       " 'like': 39,\n",
       " 'if': 40,\n",
       " 'from': 41,\n",
       " 'now': 42,\n",
       " 'our': 43,\n",
       " 'he': 44,\n",
       " 'an': 45,\n",
       " 'just': 46,\n",
       " 'these': 47,\n",
       " 'or': 48,\n",
       " 'when': 49,\n",
       " 'because': 50,\n",
       " 'very': 51,\n",
       " 'me': 52,\n",
       " 'out': 53,\n",
       " 'by': 54,\n",
       " 'them': 55,\n",
       " 'how': 56,\n",
       " 'know': 57,\n",
       " 'up': 58,\n",
       " 'going': 59,\n",
       " 'had': 60,\n",
       " 'more': 61,\n",
       " 'think': 62,\n",
       " 'who': 63,\n",
       " 'were': 64,\n",
       " 'see': 65,\n",
       " 'your': 66,\n",
       " 'their': 67,\n",
       " 'which': 68,\n",
       " 'would': 69,\n",
       " 'here': 70,\n",
       " 'really': 71,\n",
       " 'get': 72,\n",
       " 've': 73,\n",
       " 'then': 74,\n",
       " 'm': 75,\n",
       " 'world': 76,\n",
       " 'us': 77,\n",
       " 'time': 78,\n",
       " 'some': 79,\n",
       " 'has': 80,\n",
       " 'don': 81,\n",
       " 'actually': 82,\n",
       " 'into': 83,\n",
       " 'way': 84,\n",
       " 'where': 85,\n",
       " 'will': 86,\n",
       " 'years': 87,\n",
       " 'things': 88,\n",
       " 'other': 89,\n",
       " 'no': 90,\n",
       " 'could': 91,\n",
       " 'go': 92,\n",
       " 'well': 93,\n",
       " 'want': 94,\n",
       " 'been': 95,\n",
       " 'make': 96,\n",
       " 'right': 97,\n",
       " 'she': 98,\n",
       " 'said': 99,\n",
       " 'something': 100,\n",
       " 'those': 101,\n",
       " 'first': 102,\n",
       " 'two': 103,\n",
       " 'than': 104,\n",
       " 'much': 105,\n",
       " 'also': 106,\n",
       " 'look': 107,\n",
       " 'new': 108,\n",
       " 'thing': 109,\n",
       " 'little': 110,\n",
       " 'got': 111,\n",
       " 'back': 112,\n",
       " 'over': 113,\n",
       " 'most': 114,\n",
       " 'say': 115,\n",
       " 'even': 116,\n",
       " 'his': 117,\n",
       " 'life': 118,\n",
       " 'only': 119,\n",
       " 'work': 120,\n",
       " 'many': 121,\n",
       " 'take': 122,\n",
       " 'need': 123,\n",
       " 'did': 124,\n",
       " 'lot': 125,\n",
       " 'kind': 126,\n",
       " 'why': 127,\n",
       " 'good': 128,\n",
       " 'around': 129,\n",
       " 'every': 130,\n",
       " 'different': 131,\n",
       " 'down': 132,\n",
       " 'll': 133,\n",
       " 'let': 134,\n",
       " 'her': 135,\n",
       " 'through': 136,\n",
       " 'same': 137,\n",
       " 'being': 138,\n",
       " 'come': 139,\n",
       " 'd': 140,\n",
       " 'day': 141,\n",
       " 'year': 142,\n",
       " 'three': 143,\n",
       " 'use': 144,\n",
       " 'doing': 145,\n",
       " 'put': 146,\n",
       " 'called': 147,\n",
       " 'any': 148,\n",
       " 'today': 149,\n",
       " 'percent': 150,\n",
       " 'made': 151,\n",
       " 'after': 152,\n",
       " 'thank': 153,\n",
       " 'tell': 154,\n",
       " 'great': 155,\n",
       " 'human': 156,\n",
       " 'find': 157,\n",
       " 'didn': 158,\n",
       " 'fact': 159,\n",
       " 'talk': 160,\n",
       " 'change': 161,\n",
       " 'started': 162,\n",
       " 'another': 163,\n",
       " 'idea': 164,\n",
       " 'big': 165,\n",
       " 'last': 166,\n",
       " 'own': 167,\n",
       " 'before': 168,\n",
       " 'its': 169,\n",
       " 'never': 170,\n",
       " 'should': 171,\n",
       " 'better': 172,\n",
       " 'give': 173,\n",
       " 'thought': 174,\n",
       " 'went': 175,\n",
       " 'might': 176,\n",
       " 'important': 177,\n",
       " '000': 178,\n",
       " 'again': 179,\n",
       " 'able': 180,\n",
       " 'together': 181,\n",
       " 'still': 182,\n",
       " 'problem': 183,\n",
       " 'off': 184,\n",
       " 'next': 185,\n",
       " 'part': 186,\n",
       " 'course': 187,\n",
       " 'system': 188,\n",
       " 'him': 189,\n",
       " 'does': 190,\n",
       " 'each': 191,\n",
       " 'start': 192,\n",
       " 'show': 193,\n",
       " 'long': 194,\n",
       " 'ago': 195,\n",
       " 'story': 196,\n",
       " 'came': 197,\n",
       " 'brain': 198,\n",
       " 'few': 199,\n",
       " 'bit': 200,\n",
       " 'between': 201,\n",
       " 'used': 202,\n",
       " 'place': 203,\n",
       " 'technology': 204,\n",
       " 'women': 205,\n",
       " 'too': 206,\n",
       " 'old': 207,\n",
       " 'mean': 208,\n",
       " 'data': 209,\n",
       " 'water': 210,\n",
       " 'looking': 211,\n",
       " 'question': 212,\n",
       " 'maybe': 213,\n",
       " 'found': 214,\n",
       " 'love': 215,\n",
       " 'doesn': 216,\n",
       " 'end': 217,\n",
       " 'example': 218,\n",
       " '10': 219,\n",
       " 'done': 220,\n",
       " 'point': 221,\n",
       " 'four': 222,\n",
       " 'real': 223,\n",
       " 'wanted': 224,\n",
       " 'ever': 225,\n",
       " 'school': 226,\n",
       " 'understand': 227,\n",
       " 'sort': 228,\n",
       " 'live': 229,\n",
       " 'call': 230,\n",
       " 'whole': 231,\n",
       " 'always': 232,\n",
       " 'children': 233,\n",
       " 'trying': 234,\n",
       " 'may': 235,\n",
       " 'person': 236,\n",
       " 'away': 237,\n",
       " 'believe': 238,\n",
       " 'feel': 239,\n",
       " 'try': 240,\n",
       " 'million': 241,\n",
       " 'working': 242,\n",
       " 'help': 243,\n",
       " 'everything': 244,\n",
       " 'five': 245,\n",
       " 'country': 246,\n",
       " 'thinking': 247,\n",
       " 'second': 248,\n",
       " 'using': 249,\n",
       " 'information': 250,\n",
       " 'money': 251,\n",
       " 'means': 252,\n",
       " 'power': 253,\n",
       " 'took': 254,\n",
       " 'times': 255,\n",
       " 'high': 256,\n",
       " 'space': 257,\n",
       " 'number': 258,\n",
       " 'kids': 259,\n",
       " 'home': 260,\n",
       " 'become': 261,\n",
       " 'create': 262,\n",
       " 'small': 263,\n",
       " 'design': 264,\n",
       " 'making': 265,\n",
       " 'best': 266,\n",
       " 'left': 267,\n",
       " 'getting': 268,\n",
       " 'future': 269,\n",
       " 'enough': 270,\n",
       " 'man': 271,\n",
       " 'quite': 272,\n",
       " 'city': 273,\n",
       " 'without': 274,\n",
       " 'sense': 275,\n",
       " 'happened': 276,\n",
       " 'comes': 277,\n",
       " 'social': 278,\n",
       " 'probably': 279,\n",
       " 'less': 280,\n",
       " 'light': 281,\n",
       " 'energy': 282,\n",
       " 'talking': 283,\n",
       " 'am': 284,\n",
       " 'building': 285,\n",
       " 'science': 286,\n",
       " 'food': 287,\n",
       " 'body': 288,\n",
       " 'told': 289,\n",
       " 'interesting': 290,\n",
       " 'ask': 291,\n",
       " 'half': 292,\n",
       " 'pretty': 293,\n",
       " 'hard': 294,\n",
       " 'play': 295,\n",
       " 'anything': 296,\n",
       " 'lives': 297,\n",
       " 'countries': 298,\n",
       " 'coming': 299,\n",
       " 'such': 300,\n",
       " 'family': 301,\n",
       " 'stuff': 302,\n",
       " 'dollars': 303,\n",
       " 'earth': 304,\n",
       " 'moment': 305,\n",
       " '20': 306,\n",
       " 'imagine': 307,\n",
       " 'across': 308,\n",
       " 'side': 309,\n",
       " 'saw': 310,\n",
       " 'while': 311,\n",
       " 'happen': 312,\n",
       " 'okay': 313,\n",
       " 'once': 314,\n",
       " 'build': 315,\n",
       " 'having': 316,\n",
       " 'men': 317,\n",
       " 'later': 318,\n",
       " 'experience': 319,\n",
       " 'asked': 320,\n",
       " 'makes': 321,\n",
       " 'living': 322,\n",
       " 'says': 323,\n",
       " 'seen': 324,\n",
       " 'room': 325,\n",
       " 'hand': 326,\n",
       " 'simple': 327,\n",
       " 'health': 328,\n",
       " 'ways': 329,\n",
       " 'else': 330,\n",
       " 'case': 331,\n",
       " 'almost': 332,\n",
       " 'yet': 333,\n",
       " 'young': 334,\n",
       " 'days': 335,\n",
       " 'nothing': 336,\n",
       " 'bad': 337,\n",
       " 'care': 338,\n",
       " 'happens': 339,\n",
       " 'goes': 340,\n",
       " 'move': 341,\n",
       " 'states': 342,\n",
       " 'reason': 343,\n",
       " 'computer': 344,\n",
       " 'open': 345,\n",
       " 'africa': 346,\n",
       " 'learn': 347,\n",
       " 'process': 348,\n",
       " 'inside': 349,\n",
       " 'someone': 350,\n",
       " 'six': 351,\n",
       " 'far': 352,\n",
       " 'mind': 353,\n",
       " 'project': 354,\n",
       " 'remember': 355,\n",
       " 'single': 356,\n",
       " 'picture': 357,\n",
       " 'both': 358,\n",
       " 'whether': 359,\n",
       " 'problems': 360,\n",
       " 'basically': 361,\n",
       " 'community': 362,\n",
       " 'saying': 363,\n",
       " 'already': 364,\n",
       " 'within': 365,\n",
       " 'looked': 366,\n",
       " 'myself': 367,\n",
       " 'billion': 368,\n",
       " 'often': 369,\n",
       " 'possible': 370,\n",
       " 'business': 371,\n",
       " 'planet': 372,\n",
       " 'global': 373,\n",
       " 'everybody': 374,\n",
       " 'top': 375,\n",
       " 'public': 376,\n",
       " 'sure': 377,\n",
       " 'set': 378,\n",
       " 'wrong': 379,\n",
       " 'book': 380,\n",
       " 'car': 381,\n",
       " 'keep': 382,\n",
       " 'answer': 383,\n",
       " 'yes': 384,\n",
       " 'oh': 385,\n",
       " 'hope': 386,\n",
       " 'sometimes': 387,\n",
       " 'history': 388,\n",
       " 'true': 389,\n",
       " 'war': 390,\n",
       " 'child': 391,\n",
       " 'guy': 392,\n",
       " 'instead': 393,\n",
       " 'months': 394,\n",
       " 'ideas': 395,\n",
       " 'looks': 396,\n",
       " 'matter': 397,\n",
       " 'government': 398,\n",
       " 'amazing': 399,\n",
       " 'united': 400,\n",
       " 'since': 401,\n",
       " 'age': 402,\n",
       " 'bring': 403,\n",
       " 'cells': 404,\n",
       " 'job': 405,\n",
       " 'heard': 406,\n",
       " 'until': 407,\n",
       " 'face': 408,\n",
       " 'wasn': 409,\n",
       " '100': 410,\n",
       " 'read': 411,\n",
       " 'control': 412,\n",
       " 'isn': 413,\n",
       " 'research': 414,\n",
       " '30': 415,\n",
       " 'words': 416,\n",
       " 'u': 417,\n",
       " 'group': 418,\n",
       " 'under': 419,\n",
       " 'self': 420,\n",
       " 'somebody': 421,\n",
       " 'built': 422,\n",
       " 'state': 423,\n",
       " 'woman': 424,\n",
       " 'turn': 425,\n",
       " 'beautiful': 426,\n",
       " 'friends': 427,\n",
       " 'line': 428,\n",
       " 'knew': 429,\n",
       " 'couple': 430,\n",
       " 'order': 431,\n",
       " 'form': 432,\n",
       " 'yeah': 433,\n",
       " 'internet': 434,\n",
       " 'middle': 435,\n",
       " 'music': 436,\n",
       " 'piece': 437,\n",
       " 'nature': 438,\n",
       " 'head': 439,\n",
       " 'though': 440,\n",
       " 'stop': 441,\n",
       " 'everyone': 442,\n",
       " '50': 443,\n",
       " 'places': 444,\n",
       " 'video': 445,\n",
       " 'language': 446,\n",
       " 'learned': 447,\n",
       " 'run': 448,\n",
       " 'night': 449,\n",
       " 'decided': 450,\n",
       " 'study': 451,\n",
       " 'word': 452,\n",
       " 'cancer': 453,\n",
       " 'taking': 454,\n",
       " 'works': 455,\n",
       " 'became': 456,\n",
       " 'exactly': 457,\n",
       " 'species': 458,\n",
       " 'completely': 459,\n",
       " 'society': 460,\n",
       " 'education': 461,\n",
       " 'against': 462,\n",
       " 'stories': 463,\n",
       " 'large': 464,\n",
       " 'share': 465,\n",
       " 'level': 466,\n",
       " 'heart': 467,\n",
       " 'america': 468,\n",
       " 'model': 469,\n",
       " 'gets': 470,\n",
       " 'questions': 471,\n",
       " 'mother': 472,\n",
       " 'god': 473,\n",
       " 'company': 474,\n",
       " 'turns': 475,\n",
       " 'ourselves': 476,\n",
       " 'happening': 477,\n",
       " 'art': 478,\n",
       " 'hear': 479,\n",
       " 'themselves': 480,\n",
       " 'must': 481,\n",
       " 'itself': 482,\n",
       " 'kinds': 483,\n",
       " 'rather': 484,\n",
       " 'students': 485,\n",
       " 'name': 486,\n",
       " 'hours': 487,\n",
       " 'disease': 488,\n",
       " 'front': 489,\n",
       " 'house': 490,\n",
       " 'couldn': 491,\n",
       " 'huge': 492,\n",
       " 'created': 493,\n",
       " 'universe': 494,\n",
       " 'ok': 495,\n",
       " 'animals': 496,\n",
       " 'american': 497,\n",
       " 'environment': 498,\n",
       " 'worked': 499,\n",
       " 'minutes': 500,\n",
       " 'ones': 501,\n",
       " 'black': 502,\n",
       " '1': 503,\n",
       " 'perhaps': 504,\n",
       " 'past': 505,\n",
       " 'third': 506,\n",
       " 'along': 507,\n",
       " 'finally': 508,\n",
       " 'others': 509,\n",
       " 'early': 510,\n",
       " 'sound': 511,\n",
       " 'game': 512,\n",
       " 'thousands': 513,\n",
       " 'century': 514,\n",
       " 'based': 515,\n",
       " 'least': 516,\n",
       " 'per': 517,\n",
       " 'ted': 518,\n",
       " 'lots': 519,\n",
       " 'figure': 520,\n",
       " 'free': 521,\n",
       " 'guys': 522,\n",
       " 'particular': 523,\n",
       " 'happy': 524,\n",
       " 'news': 525,\n",
       " 'learning': 526,\n",
       " 'entire': 527,\n",
       " 'won': 528,\n",
       " 'gave': 529,\n",
       " 'india': 530,\n",
       " 'machine': 531,\n",
       " 'during': 532,\n",
       " 'systems': 533,\n",
       " 'air': 534,\n",
       " 'difference': 535,\n",
       " 'outside': 536,\n",
       " 'natural': 537,\n",
       " 'taken': 538,\n",
       " 'seven': 539,\n",
       " 'changed': 540,\n",
       " 'given': 541,\n",
       " 'leave': 542,\n",
       " 'cell': 543,\n",
       " '15': 544,\n",
       " 'close': 545,\n",
       " 'behind': 546,\n",
       " 'cities': 547,\n",
       " 'full': 548,\n",
       " 'scale': 549,\n",
       " 'china': 550,\n",
       " 'difficult': 551,\n",
       " 'takes': 552,\n",
       " 'companies': 553,\n",
       " 'area': 554,\n",
       " 'yourself': 555,\n",
       " 'reality': 556,\n",
       " 'seeing': 557,\n",
       " 'easy': 558,\n",
       " 'turned': 559,\n",
       " 'cost': 560,\n",
       " 'eyes': 561,\n",
       " 'moving': 562,\n",
       " 'team': 563,\n",
       " 'population': 564,\n",
       " 'culture': 565,\n",
       " 'york': 566,\n",
       " 'hands': 567,\n",
       " 'began': 568,\n",
       " 'whatever': 569,\n",
       " 'needs': 570,\n",
       " 'terms': 571,\n",
       " 'image': 572,\n",
       " 'needed': 573,\n",
       " 'simply': 574,\n",
       " 'beginning': 575,\n",
       " 'father': 576,\n",
       " 'local': 577,\n",
       " 'realized': 578,\n",
       " 'media': 579,\n",
       " 'death': 580,\n",
       " 'parents': 581,\n",
       " 'view': 582,\n",
       " 'walk': 583,\n",
       " 'white': 584,\n",
       " 'market': 585,\n",
       " 'parts': 586,\n",
       " 'ocean': 587,\n",
       " 'economic': 588,\n",
       " 'eight': 589,\n",
       " 'powerful': 590,\n",
       " 'known': 591,\n",
       " 'week': 592,\n",
       " 'size': 593,\n",
       " 'felt': 594,\n",
       " 'humans': 595,\n",
       " 'certain': 596,\n",
       " 'spend': 597,\n",
       " 'longer': 598,\n",
       " 'phone': 599,\n",
       " 'wonderful': 600,\n",
       " 'cannot': 601,\n",
       " 'common': 602,\n",
       " 'grow': 603,\n",
       " 'center': 604,\n",
       " 'tried': 605,\n",
       " 'fish': 606,\n",
       " 'land': 607,\n",
       " 'oil': 608,\n",
       " 'deal': 609,\n",
       " 'interested': 610,\n",
       " 'political': 611,\n",
       " 'red': 612,\n",
       " 'gone': 613,\n",
       " 'amount': 614,\n",
       " 'weeks': 615,\n",
       " 'opportunity': 616,\n",
       " 'spent': 617,\n",
       " 'lost': 618,\n",
       " 'paper': 619,\n",
       " 'national': 620,\n",
       " 'quickly': 621,\n",
       " 'blue': 622,\n",
       " 'step': 623,\n",
       " 'poor': 624,\n",
       " 'green': 625,\n",
       " 'buy': 626,\n",
       " 'growth': 627,\n",
       " 'wouldn': 628,\n",
       " 'either': 629,\n",
       " 'patients': 630,\n",
       " 'sitting': 631,\n",
       " 'ability': 632,\n",
       " 'changes': 633,\n",
       " 'write': 634,\n",
       " 'challenge': 635,\n",
       " 'south': 636,\n",
       " 'low': 637,\n",
       " 'friend': 638,\n",
       " 'growing': 639,\n",
       " 'field': 640,\n",
       " '2': 641,\n",
       " 'shows': 642,\n",
       " 'born': 643,\n",
       " 'rest': 644,\n",
       " 'climate': 645,\n",
       " '40': 646,\n",
       " 'test': 647,\n",
       " 'street': 648,\n",
       " 'incredible': 649,\n",
       " 'surface': 650,\n",
       " 'average': 651,\n",
       " 'morning': 652,\n",
       " 'pay': 653,\n",
       " 'physical': 654,\n",
       " 'program': 655,\n",
       " 'scientists': 656,\n",
       " 'value': 657,\n",
       " 'behavior': 658,\n",
       " 'feeling': 659,\n",
       " 'girl': 660,\n",
       " 'hundreds': 661,\n",
       " 'met': 662,\n",
       " 'economy': 663,\n",
       " 'dna': 664,\n",
       " 'complex': 665,\n",
       " 'access': 666,\n",
       " 'risk': 667,\n",
       " 'animal': 668,\n",
       " 'structure': 669,\n",
       " 'feet': 670,\n",
       " 'attention': 671,\n",
       " 'anyone': 672,\n",
       " 'areas': 673,\n",
       " 'deep': 674,\n",
       " 'watch': 675,\n",
       " 'short': 676,\n",
       " 'absolutely': 677,\n",
       " 'brought': 678,\n",
       " 'speak': 679,\n",
       " 'bottom': 680,\n",
       " 'die': 681,\n",
       " 'audience': 682,\n",
       " 'numbers': 683,\n",
       " 'stage': 684,\n",
       " 'realize': 685,\n",
       " 'images': 686,\n",
       " 'law': 687,\n",
       " 'wrote': 688,\n",
       " 'understanding': 689,\n",
       " 'books': 690,\n",
       " 'knowledge': 691,\n",
       " 'literally': 692,\n",
       " 'movement': 693,\n",
       " 'giving': 694,\n",
       " 'ground': 695,\n",
       " 'eat': 696,\n",
       " 'force': 697,\n",
       " 'alone': 698,\n",
       " 'seems': 699,\n",
       " 'telling': 700,\n",
       " 'hold': 701,\n",
       " 'starting': 702,\n",
       " 'nice': 703,\n",
       " 'forward': 704,\n",
       " 'sea': 705,\n",
       " 'developed': 706,\n",
       " 'individual': 707,\n",
       " 'kid': 708,\n",
       " 'millions': 709,\n",
       " 'support': 710,\n",
       " 'miles': 711,\n",
       " 'running': 712,\n",
       " 'tools': 713,\n",
       " 'online': 714,\n",
       " 'result': 715,\n",
       " 'act': 716,\n",
       " 'medical': 717,\n",
       " 'technologies': 718,\n",
       " 'north': 719,\n",
       " 'development': 720,\n",
       " 'lab': 721,\n",
       " 'blood': 722,\n",
       " 'fear': 723,\n",
       " 'map': 724,\n",
       " 'nobody': 725,\n",
       " 'personal': 726,\n",
       " 'issue': 727,\n",
       " 'voice': 728,\n",
       " 'key': 729,\n",
       " 'material': 730,\n",
       " '12': 731,\n",
       " 'recently': 732,\n",
       " 'theory': 733,\n",
       " 'cut': 734,\n",
       " 'cars': 735,\n",
       " 'fast': 736,\n",
       " 'sun': 737,\n",
       " 'changing': 738,\n",
       " 'clear': 739,\n",
       " 'playing': 740,\n",
       " 'especially': 741,\n",
       " 'girls': 742,\n",
       " 'patient': 743,\n",
       " 'soon': 744,\n",
       " 'choice': 745,\n",
       " 'fly': 746,\n",
       " 'creating': 747,\n",
       " 'discovered': 748,\n",
       " 'europe': 749,\n",
       " 'normal': 750,\n",
       " 'relationship': 751,\n",
       " 'talked': 752,\n",
       " 'gives': 753,\n",
       " 'generation': 754,\n",
       " 'dark': 755,\n",
       " 'showed': 756,\n",
       " 'type': 757,\n",
       " 'asking': 758,\n",
       " 'chance': 759,\n",
       " 'industry': 760,\n",
       " 'seem': 761,\n",
       " 'rate': 762,\n",
       " 'color': 763,\n",
       " 'designed': 764,\n",
       " 'fun': 765,\n",
       " 'hour': 766,\n",
       " 'issues': 767,\n",
       " 'class': 768,\n",
       " 'computers': 769,\n",
       " 'innovation': 770,\n",
       " 'several': 771,\n",
       " 'university': 772,\n",
       " 'english': 773,\n",
       " 'tiny': 774,\n",
       " 'allow': 775,\n",
       " 'focus': 776,\n",
       " 'save': 777,\n",
       " 'developing': 778,\n",
       " 'digital': 779,\n",
       " 'solve': 780,\n",
       " 'film': 781,\n",
       " 'network': 782,\n",
       " 'special': 783,\n",
       " 'wall': 784,\n",
       " 'baby': 785,\n",
       " 'situation': 786,\n",
       " 'solution': 787,\n",
       " 'box': 788,\n",
       " 'knows': 789,\n",
       " 'reasons': 790,\n",
       " 'dead': 791,\n",
       " 'meet': 792,\n",
       " 'stand': 793,\n",
       " 'anybody': 794,\n",
       " 'begin': 795,\n",
       " 'haven': 796,\n",
       " 'impact': 797,\n",
       " 'term': 798,\n",
       " 'beyond': 799,\n",
       " 'pictures': 800,\n",
       " 'please': 801,\n",
       " 'ice': 802,\n",
       " 'produce': 803,\n",
       " 'shape': 804,\n",
       " 'non': 805,\n",
       " 'cool': 806,\n",
       " 'resources': 807,\n",
       " 'robot': 808,\n",
       " 'groups': 809,\n",
       " 'likely': 810,\n",
       " 'stay': 811,\n",
       " 'available': 812,\n",
       " 'major': 813,\n",
       " 'rights': 814,\n",
       " 'truth': 815,\n",
       " 'cause': 816,\n",
       " 'evidence': 817,\n",
       " 'experiment': 818,\n",
       " 'obviously': 819,\n",
       " 'writing': 820,\n",
       " 'drug': 821,\n",
       " 'modern': 822,\n",
       " 'becomes': 823,\n",
       " 'incredibly': 824,\n",
       " 'aren': 825,\n",
       " 'bigger': 826,\n",
       " 'guess': 827,\n",
       " 'drugs': 828,\n",
       " 'nine': 829,\n",
       " 'product': 830,\n",
       " 'lived': 831,\n",
       " 'involved': 832,\n",
       " 'google': 833,\n",
       " 'month': 834,\n",
       " 'perfect': 835,\n",
       " 'effect': 836,\n",
       " 'putting': 837,\n",
       " 'similar': 838,\n",
       " 'eye': 839,\n",
       " 'pick': 840,\n",
       " 'message': 841,\n",
       " 'quality': 842,\n",
       " 'violence': 843,\n",
       " 'web': 844,\n",
       " 'evolution': 845,\n",
       " 'office': 846,\n",
       " 'security': 847,\n",
       " 'basic': 848,\n",
       " 'general': 849,\n",
       " 'indeed': 850,\n",
       " 'solar': 851,\n",
       " 'towards': 852,\n",
       " 'drive': 853,\n",
       " 'present': 854,\n",
       " 'listen': 855,\n",
       " 'send': 856,\n",
       " 'worth': 857,\n",
       " 'revolution': 858,\n",
       " 'explain': 859,\n",
       " 'hundred': 860,\n",
       " 'certainly': 861,\n",
       " 'died': 862,\n",
       " 'ready': 863,\n",
       " 'teach': 864,\n",
       " '25': 865,\n",
       " 'journey': 866,\n",
       " 'hit': 867,\n",
       " 'walking': 868,\n",
       " 'led': 869,\n",
       " 'approach': 870,\n",
       " 'international': 871,\n",
       " '200': 872,\n",
       " 'games': 873,\n",
       " 'scientific': 874,\n",
       " '60': 875,\n",
       " 'chinese': 876,\n",
       " 'communities': 877,\n",
       " 'crazy': 878,\n",
       " 'potential': 879,\n",
       " 'device': 880,\n",
       " 'camera': 881,\n",
       " 'source': 882,\n",
       " 'carbon': 883,\n",
       " 'rules': 884,\n",
       " 'sex': 885,\n",
       " 'sounds': 886,\n",
       " 'boy': 887,\n",
       " 'reach': 888,\n",
       " 'starts': 889,\n",
       " 'examples': 890,\n",
       " 'west': 891,\n",
       " '3': 892,\n",
       " 'measure': 893,\n",
       " 'particularly': 894,\n",
       " 'software': 895,\n",
       " 'success': 896,\n",
       " 'totally': 897,\n",
       " 'suddenly': 898,\n",
       " 'action': 899,\n",
       " 'college': 900,\n",
       " 'higher': 901,\n",
       " 'code': 902,\n",
       " 'eventually': 903,\n",
       " 'democracy': 904,\n",
       " 'among': 905,\n",
       " 'minute': 906,\n",
       " 'sit': 907,\n",
       " 'largest': 908,\n",
       " 'hospital': 909,\n",
       " 'showing': 910,\n",
       " 'notice': 911,\n",
       " 'period': 912,\n",
       " 'develop': 913,\n",
       " 'mass': 914,\n",
       " 'dream': 915,\n",
       " 'onto': 916,\n",
       " 'add': 917,\n",
       " 'break': 918,\n",
       " 'memory': 919,\n",
       " 'plants': 920,\n",
       " '11': 921,\n",
       " 'everywhere': 922,\n",
       " 'favorite': 923,\n",
       " 'zero': 924,\n",
       " 'moved': 925,\n",
       " 'response': 926,\n",
       " 'schools': 927,\n",
       " 'speed': 928,\n",
       " 'wants': 929,\n",
       " 'extremely': 930,\n",
       " 'individuals': 931,\n",
       " 'movie': 932,\n",
       " 'table': 933,\n",
       " 'wait': 934,\n",
       " 'anyway': 935,\n",
       " 'follow': 936,\n",
       " 'medicine': 937,\n",
       " 'trust': 938,\n",
       " 'biggest': 939,\n",
       " 'plant': 940,\n",
       " 'organization': 941,\n",
       " 'watching': 942,\n",
       " 'creative': 943,\n",
       " 'road': 944,\n",
       " 'choose': 945,\n",
       " 'worse': 946,\n",
       " '500': 947,\n",
       " 'lead': 948,\n",
       " 'results': 949,\n",
       " 'exciting': 950,\n",
       " 'grew': 951,\n",
       " 'materials': 952,\n",
       " 'jobs': 953,\n",
       " 'east': 954,\n",
       " 'plan': 955,\n",
       " 'positive': 956,\n",
       " 'poverty': 957,\n",
       " 'fall': 958,\n",
       " 'safe': 959,\n",
       " 'strong': 960,\n",
       " 'essentially': 961,\n",
       " 'door': 962,\n",
       " 'object': 963,\n",
       " 'continue': 964,\n",
       " 'further': 965,\n",
       " 'happiness': 966,\n",
       " 'son': 967,\n",
       " 'vision': 968,\n",
       " 'doctor': 969,\n",
       " 'student': 970,\n",
       " 'including': 971,\n",
       " 'named': 972,\n",
       " 'objects': 973,\n",
       " 'role': 974,\n",
       " 'extraordinary': 975,\n",
       " 'standing': 976,\n",
       " 'african': 977,\n",
       " 'conversation': 978,\n",
       " 'leaders': 979,\n",
       " 'projects': 980,\n",
       " 'skin': 981,\n",
       " 'usually': 982,\n",
       " 'allowed': 983,\n",
       " 'models': 984,\n",
       " 'supposed': 985,\n",
       " 'faster': 986,\n",
       " 'interest': 987,\n",
       " 'families': 988,\n",
       " 'fight': 989,\n",
       " 'police': 990,\n",
       " 'connected': 991,\n",
       " 'tree': 992,\n",
       " '5': 993,\n",
       " 'cases': 994,\n",
       " 'screen': 995,\n",
       " 'goal': 996,\n",
       " 'somewhere': 997,\n",
       " 'buildings': 998,\n",
       " 'finding': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词向量矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21449, 300)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_embeddings(embed_index, vocab, n_dim):\n",
    "    \"\"\"根据词汇表获取embedding中的词向量\"\"\"\n",
    "    embedding_matrix = nd.zeros((len(vocab), n_dim))\n",
    "    for word, i in vocab.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embed_index.get_vecs_by_tokens(word)\n",
    "        except:\n",
    "            pass\n",
    "    return embedding_matrix\n",
    "\n",
    "embed_matrix = build_embeddings(fasttext_embedding, vocab, 300)\n",
    "embed_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获得固定长度的文本序列及将词替换为index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def pad_sentences(sentences, padding_word=\"<UNK>\"):\n",
    "    \"\"\"\n",
    "    将文本处理为固定长度，长度为最长文档的长度\n",
    "    \"\"\"\n",
    "    sequence_length = int(0.8 * max(len(x) for x in sentences))\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        if len(sentence) >= sequence_length:\n",
    "            new_sentence = sentence[0:sequence_length]\n",
    "        else:\n",
    "            num_padding = sequence_length - len(sentence)\n",
    "            new_sentence = sentence + [padding_word] * num_padding\n",
    "        padded_sentences.append(new_sentence)\n",
    "    return padded_sentences\n",
    "\n",
    "def build_input_data(sentences, labels, vocabulary):\n",
    "    \"\"\"\n",
    "    基于词典映射句子和标签\n",
    "    \"\"\"\n",
    "    x = np.array([[vocabulary[word] if word in vocabulary else 0 for word in sentence] for sentence in sentences])\n",
    "    y = np.zeros(shape=(len(labels), 8))\n",
    "    le = preprocessing.LabelEncoder() \n",
    "    labels = le.fit_transform(new_labels)\n",
    "    for i,label in enumerate(labels):\n",
    "        y[i][label] = 1\n",
    "    return [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  70   19  103  790  553 2037   16  119   34   61]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "(2085, 5348)\n"
     ]
    }
   ],
   "source": [
    "sentences_padded = pad_sentences(new_text)\n",
    "x, y = build_input_data(sentences_padded, labels, vocab)\n",
    "print(x[0][0:10])\n",
    "print(y[0])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "切分训练/验证/测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集/验证集比例: 1585/250\n",
      "训练集大小: (1585, 5348)\n",
      "验证集大小: (250, 5348)\n",
      "词汇量 21449\n",
      "最长的文档字数 5348\n"
     ]
    }
   ],
   "source": [
    "# 随机打乱数据\n",
    "np.random.seed(2018)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "x_train, x_dev = x_shuffled[:-500], x_shuffled[-500:]\n",
    "y_train, y_dev = y_shuffled[:-500], y_shuffled[-500:]\n",
    "x_dev, x_test = x_dev[:-250], x_dev[-250:]\n",
    "y_dev, y_test = y_dev[:-250], y_dev[-250:]\n",
    "sentence_size = x_train.shape[1]\n",
    "                              \n",
    "print('训练集/验证集比例: %d/%d' % (len(y_train), len(y_dev)))\n",
    "print('训练集大小:', x_train.shape)\n",
    "print('验证集大小:', x_dev.shape)\n",
    "print('词汇量', vocab_size)\n",
    "print('最长的文档字数', sentence_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 8214528 into shape (64,24)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-050d68eda173>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatchify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-050d68eda173>\u001b[0m in \u001b[0;36mbatchify\u001b[1;34m(data, batch_size)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mnum_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnum_batches\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 8214528 into shape (64,24)"
     ]
    }
   ],
   "source": [
    "def batchify(data, batch_size):\n",
    "    \"\"\"数据形状 (num_batches, batch_size)\"\"\"\n",
    "    num_batches = data.shape[0] // batch_size\n",
    "    data = data[:num_batches * batch_size]\n",
    "    data = data.reshape((batch_size, num_batches)).T\n",
    "    return data\n",
    "\n",
    "data = batchify(x_train, batch_size)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def try_gpu():\n",
    "    \"\"\"如果gpu可用, 使用第一个gpu设备; 否则使用CPU\"\"\"\n",
    "    try:\n",
    "        ctx = mx.gpu()\n",
    "        _ = nd.array([0], ctx=ctx)\n",
    "    except:\n",
    "        ctx = mx.cpu()\n",
    "    return ctx\n",
    "context = try_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建模型，定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_fast = RNNModel(model_name, fasttext_embedding, vocab_size, \n",
    "                      embed_dim, hidden_dim, num_layers, num_labels, dropout_rate)\n",
    "model_fast.collect_params().initialize(mx.init.Xavier(), ctx=context) # xavier权重值初始化\n",
    "trainer = gluon.Trainer(model_fast.collect_params(), \n",
    "                        'adam', {'learning_rate': lr}) # adam优化器\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss() # 交叉熵loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型大致结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Block.name_scope of RNNModel(\n",
       "  (drop): Dropout(p = 0.5)\n",
       "  (encoder): Embedding(21449 -> 300, float32)\n",
       "  (rnn): GRU(300 -> 300, TNC, num_layers=2, dropout=0.5, bidirectional)\n",
       "  (decoder): Dense(100 -> 8, Activation(softmax))\n",
       ")>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fast.name_scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(context, mx.Context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, context):\n",
    "    print(\"Start training on \", context)\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc, n, m = 0.0, 0.0, 0.0, 0.0\n",
    "        start_time = time.time()\n",
    "        hidden = model.begin_state(func = mx.nd.zeros, batch_size = batch_size,\n",
    "                                   ctx = context)\n",
    "        for begin in enumerate(range(0, x_train.shape[0], batch_size)):\n",
    "            batchX = x_train[begin:begin+batch_size]\n",
    "            batchY = y_train[begin:begin+batch_size]\n",
    "            if epoch > 0 and epoch % lr_period == 0:\n",
    "                trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "            with autograd.record():\n",
    "                output, hidden = model(data, hidden)\n",
    "                L = loss(output, target)\n",
    "                L.backward()\n",
    "\n",
    "            grads = [i.grad(context) for i in model.collect_params().values()]\n",
    "            # 梯度裁剪。需要注意的是，这里的梯度是整个批量的梯度。\n",
    "            # 因此我们将clipping_norm乘以num_steps和batch_size。\n",
    "            gluon.utils.clip_global_norm(grads,\n",
    "                                         clipping_norm * num_steps * batch_size)\n",
    "\n",
    "            trainer.step(batch_size)\n",
    "            total_L += mx.nd.sum(L).asscalar()\n",
    "\n",
    "            if ibatch % eval_period == 0 and ibatch > 0:\n",
    "                cur_L = total_L / num_steps / batch_size / eval_period\n",
    "                print('[Epoch %d Batch %d] loss %.2f, perplexity %.2f' % (\n",
    "                    epoch + 1, ibatch, cur_L, math.exp(cur_L)))\n",
    "                total_L = 0.0\n",
    "\n",
    "        val_L = model_eval(val_data)\n",
    "\n",
    "        print('[Epoch %d] time cost %.2fs, validation loss %.2f, validation '\n",
    "              'perplexity %.2f' % (epoch + 1, time.time() - start_time, val_L,\n",
    "                                   math.exp(val_L)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 比较使用不同词向量的模型的学习曲线，随机词向量，变化的glove词向量和不变的glove词向量，哪一个模型在测试集上表现最好？哪个模型的准确率最高？\n",
    "\n",
    "2. 如果我们将tanh换成sigmoid或者relu等非线性函数，结果会怎样？\n",
    "\n",
    "3. 如果你加入dropout层，结果会怎样？\n",
    "\n",
    "4. 如果你改变隐藏层大小，结果会怎样？\n",
    "\n",
    "5. 如果你想再加入一层隐藏层，代码应作何修改？\n",
    "\n",
    "6. 训练算法对模型的质量的影响程度？\n",
    "\n",
    "7. 如果我们队标签映射为2维的向量并可视化，会有什么有趣的情况发生？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 额外部分：多标签分类"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
